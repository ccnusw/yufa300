import streamlit as st
import os
import docx
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain_core.prompts import PromptTemplate
# ç§»é™¤ä¸å†éœ€è¦çš„ 'import google.generativeai as genai'

# --- å¸¸é‡å®šä¹‰ ---
FAISS_INDEX_PATH = "faiss_index"

# --- UIç•Œé¢è®¾è®¡ ---
st.set_page_config(page_title="ç°ä»£æ±‰è¯­è¯­æ³•ä¸‰ç™¾é—®æ™ºæ€ä½“æ£€ç´¢ç³»ç»Ÿ", layout="wide")
st.title("ğŸ“– ç°ä»£æ±‰è¯­è¯­æ³•ä¸‰ç™¾é—®æ™ºæ€ä½“æ£€ç´¢ç³»ç»Ÿ")

# --- API KeyåŠæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿæ ¸å¿ƒè®¾ç½®")
    api_key_input = st.text_input(
        "è¯·è¾“å…¥æ‚¨çš„Google API Key:",
        type="password",
        help="ç³»ç»Ÿä¹Ÿä¼šè‡ªåŠ¨æ£€æµ‹'GOOGLE_API_KEY'ç¯å¢ƒå˜é‡ã€‚"
    )

    final_api_key = api_key_input if api_key_input else os.getenv("GOOGLE_API_KEY")

if not final_api_key:
    st.error("âŒ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥æ‚¨çš„Google API Keyæˆ–è®¾ç½®'GOOGLE_API_KEY'ç¯å¢ƒå˜é‡ã€‚")
    st.stop()

# --- åˆå§‹åŒ–Embeddingsæ¨¡å‹ ---
# å°†API Keyç›´æ¥ä¼ é€’ç»™æ„é€ å‡½æ•°ï¼Œç¡®ä¿å…¶è¢«æ­£ç¡®ä½¿ç”¨
try:
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        task_type="RETRIEVAL_QUERY",
        google_api_key=final_api_key
    )
except Exception as e:
    st.error(f"Embeddingsæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„API Keyã€‚é”™è¯¯ä¿¡æ¯: {e}")
    st.stop()


# --- çŸ¥è¯†åº“å¤„ç†æ ¸å¿ƒå‡½æ•° ---
def process_and_save_document(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„Wordæ–‡æ¡£ï¼Œåˆ›å»ºå¹¶ä¿å­˜å‘é‡ç´¢å¼•"""
    with st.spinner("é¦–æ¬¡åˆå§‹åŒ–ï¼šæ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºå‘é‡çŸ¥è¯†åº“..."):
        try:
            doc = docx.Document(uploaded_file)
            full_text = "\n".join([para.text for para in doc.paragraphs if para.text])

            if not full_text.strip():
                st.error("ä¸Šä¼ çš„æ–‡æ¡£ä¸ºç©ºæˆ–ä¸åŒ…å«ä»»ä½•æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥åé‡æ–°ä¸Šä¼ ã€‚")
                return

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(full_text)

            # ä½¿ç”¨å·²ç»åˆå§‹åŒ–å¥½çš„embeddingsæ¨¡å‹
            vector_store = FAISS.from_texts(chunks, embedding=embeddings)
            vector_store.save_local(FAISS_INDEX_PATH)

        except Exception as e:
            st.error(f"å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            if os.path.exists(FAISS_INDEX_PATH):
                import shutil
                shutil.rmtree(FAISS_INDEX_PATH)


@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½å‘é‡çŸ¥è¯†åº“...")
def load_vector_store():
    """ä»æœ¬åœ°åŠ è½½å‘é‡ç´¢å¼•"""
    try:
        # åŠ è½½æ—¶ä¹Ÿéœ€è¦ä¼ å…¥embeddingså®ä¾‹
        return FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
    except Exception as e:
        st.error(f"åŠ è½½æœ¬åœ°å‘é‡çŸ¥è¯†åº“å¤±è´¥: {e}. å¦‚æœæ‚¨åˆšåˆšä¸Šä¼ äº†æ–°æ–‡ä»¶ï¼Œè¯·åˆ·æ–°é¡µé¢ã€‚")
        return None


# --- ä¸»é€»è¾‘ ---
vector_store = None

if os.path.exists(FAISS_INDEX_PATH):
    st.sidebar.success("âœ… å‘é‡çŸ¥è¯†åº“å·²åŠ è½½ï¼Œç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
    vector_store = load_vector_store()
else:
    st.sidebar.warning("âš ï¸ ç³»ç»Ÿä¸­æœªæ‰¾åˆ°å‘é‡çŸ¥è¯†åº“ã€‚")
    st.sidebar.markdown("---")
    st.sidebar.header("é¦–æ¬¡åˆå§‹åŒ–")
    uploaded_file = st.sidebar.file_uploader(
        "è¯·ä¸Šä¼ åˆå§‹çš„WordçŸ¥è¯†åº“æ–‡æ¡£ (.docx)",
        type="docx"
    )

    if uploaded_file is not None:
        process_and_save_document(uploaded_file)
        st.sidebar.success("çŸ¥è¯†åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜ï¼é¡µé¢å°†è‡ªåŠ¨åˆ·æ–°ä»¥åŠ è½½ã€‚")
        st.rerun()
    else:
        st.info("è¯·â€œç®¡ç†å‘˜â€åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ªWordæ–‡æ¡£æ¥æ„å»ºç³»ç»ŸçŸ¥è¯†åº“ã€‚")
        st.stop()

# --- èŠå¤©ç•Œé¢ ---
if vector_store:
    st.markdown("""
    æ¬¢è¿ä½¿ç”¨æœ¬ç³»ç»Ÿï¼çŸ¥è¯†åº“å·²å‡†å¤‡å¥½ï¼Œæ‚¨ç°åœ¨å¯ä»¥åœ¨ä¸‹æ–¹çš„èŠå¤©æ¡†ä¸­å¼€å§‹æé—®ã€‚
    """)
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_question := st.chat_input("è¯·å°±ç°ä»£æ±‰è¯­è¯­æ³•è¿›è¡Œæé—®..."):
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– æ™ºæ€ä½“æ­£åœ¨æ£€ç´¢ä¸æ€è€ƒä¸­..."):
                docs = vector_store.similarity_search(user_question, k=3)

                if not docs:
                    response = "æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"
                else:
                    prompt_template = """
                    è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚
                    ç¡®ä¿ä½ çš„å›ç­”å®Œå…¨åŸºäºè¿™äº›ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€‚
                    å¦‚æœæ ¹æ®ä¸Šä¸‹æ–‡æ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´ï¼šâ€œæŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚â€

                    ä¸Šä¸‹æ–‡:
                    {context}

                    é—®é¢˜:
                    {question}

                    å›ç­”:
                    """
                    prompt = PromptTemplate(
                        template=prompt_template, input_variables=["context", "question"]
                    )
                    # æ˜¾å¼ä¼ é€’API Keyå¹¶ä½¿ç”¨æ–°çš„Flashæ¨¡å‹
                    model = ChatGoogleGenerativeAI(
                        model="gemini-1.5-flash-latest",
                        temperature=0.3,
                        google_api_key=final_api_key
                    )
                    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)

                    response_obj = chain.invoke(
                        {"input_documents": docs, "question": user_question},
                        return_only_outputs=True
                    )
                    response = response_obj['output_text']

                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
