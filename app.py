import streamlit as st
import os
import docx
import time  # å¯¼å…¥ time æ¨¡å—
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain_core.prompts import PromptTemplate
import google.generativeai as genai

# --- å¸¸é‡å®šä¹‰ ---
FAISS_INDEX_PATH = "faiss_index"

# --- UIç•Œé¢è®¾è®¡ ---
st.set_page_config(page_title="ç°ä»£æ±‰è¯­è¯­æ³•ä¸‰ç™¾é—®æ™ºæ€ä½“æ£€ç´¢ç³»ç»Ÿ", layout="wide")
st.title("ğŸ“– ç°ä»£æ±‰è¯­è¯­æ³•ä¸‰ç™¾é—®æ™ºæ€ä½“æ£€ç´¢ç³»ç»Ÿ")

# --- API KeyåŠæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿæ ¸å¿ƒè®¾ç½®")
    api_key_input = st.text_input(
        "è¯·è¾“å…¥æ‚¨çš„Google API Key:",
        type="password",
        help="ç³»ç»Ÿä¹Ÿä¼šè‡ªåŠ¨æ£€æµ‹'GOOGLE_API_KEY'ç¯å¢ƒå˜é‡ã€‚"
    )

    final_api_key = api_key_input if api_key_input else os.getenv("GOOGLE_API_KEY")

if not final_api_key:
    st.error("âŒ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥æ‚¨çš„Google API Keyæˆ–è®¾ç½®'GOOGLE_API_KEY'ç¯å¢ƒå˜é‡ã€‚")
    st.stop()

try:
    genai.configure(api_key=final_api_key)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", task_type="RETRIEVAL_QUERY")
except Exception as e:
    st.error(f"API Keyé…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„Keyæ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    st.stop()


# --- çŸ¥è¯†åº“å¤„ç†æ ¸å¿ƒå‡½æ•° (ä¼˜åŒ–ç‰ˆï¼ŒåŒ…å«åˆ†æ‰¹å¤„ç†å’Œé‡è¯•) ---
def process_and_save_document(uploaded_file):
    with st.spinner("é¦–æ¬¡åˆå§‹åŒ–ï¼šæ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºå‘é‡çŸ¥è¯†åº“..."):
        try:
            # 1. è¯»å–å’Œåˆ‡åˆ†æ–‡æ¡£
            doc = docx.Document(uploaded_file)
            full_text = "\n".join([para.text for para in doc.paragraphs if para.text])

            if not full_text.strip():
                st.error("ä¸Šä¼ çš„æ–‡æ¡£ä¸ºç©ºæˆ–ä¸åŒ…å«ä»»ä½•æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥åé‡æ–°ä¸Šä¼ ã€‚")
                return

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(full_text)

            if not chunks:
                st.error("æ–‡æ¡£åˆ‡åˆ†åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹ã€‚")
                return

            # è®¾ç½®æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°
            batch_size = 100

            # 2. ç”¨ç¬¬ä¸€æ‰¹çš„æ–‡æœ¬å—åˆå§‹åŒ–FAISSç´¢å¼•
            st.info(f"æ£€æµ‹åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚å¼€å§‹åˆ†æ‰¹æ„å»ºçŸ¥è¯†åº“...")
            vector_store = FAISS.from_texts(
                texts=chunks[:batch_size],
                embedding=embeddings
            )
            st.success(f"å·²æˆåŠŸå¤„ç†å‰ {min(batch_size, len(chunks))}/{len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

            # 3. å¾ªç¯å¤„ç†å‰©ä½™çš„æ–‡æœ¬å—
            for i in range(batch_size, len(chunks), batch_size):
                retries = 3
                while retries > 0:
                    try:
                        batch = chunks[i:i + batch_size]
                        vector_store.add_texts(texts=batch)  # LangChain v0.2+ æ¨èä¸æ˜¾å¼ä¼ é€’embedding
                        st.success(f"å·²æˆåŠŸå¤„ç† {min(i + batch_size, len(chunks))}/{len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
                        time.sleep(1)
                        break
                    except Exception as e:
                        retries -= 1
                        st.warning(f"å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}ã€‚å‰©ä½™é‡è¯•æ¬¡æ•°: {retries}ã€‚æ­£åœ¨ç­‰å¾…åé‡è¯•...")
                        time.sleep(5)
                if retries == 0:
                    st.error(f"æ‰¹æ¬¡ {i // batch_size + 1} é‡è¯•å¤šæ¬¡åä»ç„¶å¤±è´¥ï¼ŒçŸ¥è¯†åº“æ„å»ºä¸­æ­¢ã€‚")
                    raise Exception("Failed to process document in batches.")

            # 4. ä¿å­˜æœ€ç»ˆçš„æœ¬åœ°ç´¢å¼•
            vector_store.save_local(FAISS_INDEX_PATH)
            st.info("æ‰€æœ‰æ–‡æœ¬å—å¤„ç†å®Œæ¯•ï¼Œå‘é‡çŸ¥è¯†åº“å·²æˆåŠŸä¿å­˜ï¼")

        except Exception as e:
            st.error(f"å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            if os.path.exists(FAISS_INDEX_PATH):
                import shutil
                shutil.rmtree(FAISS_INDEX_PATH)


@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½å‘é‡çŸ¥è¯†åº“...")
def load_vector_store():
    try:
        # åœ¨FAISS.load_localä¸­ï¼Œembeddingså¯¹è±¡æ˜¯å¿…é¡»çš„ï¼Œå› ä¸ºå®ƒéœ€è¦çŸ¥é“å¦‚ä½•å¤„ç†æŸ¥è¯¢å‘é‡
        return FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
    except Exception as e:
        st.error(f"åŠ è½½æœ¬åœ°å‘é‡çŸ¥è¯†åº“å¤±è´¥: {e}. å¦‚æœæ‚¨åˆšåˆšä¸Šä¼ äº†æ–°æ–‡ä»¶ï¼Œè¯·åˆ·æ–°é¡µé¢ã€‚")
        return None


# --- ä¸»é€»è¾‘ ---
vector_store = None

if os.path.exists(FAISS_INDEX_PATH):
    st.sidebar.success("âœ… å‘é‡çŸ¥è¯†åº“å·²åŠ è½½ï¼Œç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
    vector_store = load_vector_store()
else:
    st.sidebar.warning("âš ï¸ ç³»ç»Ÿä¸­æœªæ‰¾åˆ°å‘é‡çŸ¥è¯†åº“ã€‚")
    st.sidebar.markdown("---")
    st.sidebar.header("é¦–æ¬¡åˆå§‹åŒ–")
    uploaded_file = st.sidebar.file_uploader(
        "è¯·ä¸Šä¼ åˆå§‹çš„WordçŸ¥è¯†åº“æ–‡æ¡£ (.docx)",
        type="docx"
    )

    if uploaded_file is not None:
        process_and_save_document(uploaded_file)
        st.sidebar.success("çŸ¥è¯†åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜ï¼é¡µé¢å°†è‡ªåŠ¨åˆ·æ–°ä»¥åŠ è½½ã€‚")
        st.rerun()
    else:
        st.info("è¯·â€œç®¡ç†å‘˜â€åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ªWordæ–‡æ¡£æ¥æ„å»ºç³»ç»ŸçŸ¥è¯†åº“ã€‚")
        st.stop()

# --- èŠå¤©ç•Œé¢ ---
if vector_store:
    st.markdown("""
    æ¬¢è¿ä½¿ç”¨æœ¬ç³»ç»Ÿï¼çŸ¥è¯†åº“å·²å‡†å¤‡å¥½ï¼Œæ‚¨ç°åœ¨å¯ä»¥åœ¨ä¸‹æ–¹çš„èŠå¤©æ¡†ä¸­å¼€å§‹æé—®ã€‚
    """)
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_question := st.chat_input("è¯·å°±ç°ä»£æ±‰è¯­è¯­æ³•è¿›è¡Œæé—®..."):
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– æ™ºæ€ä½“æ­£åœ¨æ£€ç´¢ä¸æ€è€ƒä¸­..."):
                docs = vector_store.similarity_search(user_question, k=3)

                if not docs:
                    response = "æœ¬çŸ¥è¯†åº“é‡Œä¸åŒ…å«è¿™ä¸ªé—®é¢˜ã€‚"
                else:
                    question_prompt_template = """
                    è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚
                    ç¡®ä¿ä½ çš„å›ç­”å®Œå…¨åŸºäºè¿™äº›ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€‚
                    å¦‚æœæ ¹æ®ä¸Šä¸‹æ–‡æ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·æŒ‡æ˜ä¿¡æ¯ä¸è¶³ã€‚
                    ä¸Šä¸‹æ–‡:
                    {context}
                    é—®é¢˜:
                    {question}
                    å›ç­”:
                    """
                    QUESTION_PROMPT = PromptTemplate(
                        template=question_prompt_template, input_variables=["context", "question"]
                    )

                    combine_prompt_template = """
                    ç°æœ‰å¤šä¸ªä»æ–‡æ¡£ä¸­æŠ½å–çš„ç­”æ¡ˆç‰‡æ®µï¼Œè¯·å¯¹è¿™äº›ç‰‡æ®µè¿›è¡Œå…¨é¢å’Œè¯¦ç»†çš„æ•´åˆä¸æ€»ç»“ï¼Œå½¢æˆä¸€ä¸ªæœ€ç»ˆçš„ã€æµç•…ä¸”å”¯ä¸€çš„å›ç­”ã€‚
                    è¯·ä¸“æ³¨äºæ•´åˆä¿¡æ¯ï¼Œä¸è¦é‡å¤â€œæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡â€ä¹‹ç±»çš„è¯è¯­ã€‚
                    å¦‚æœä½ è®¤ä¸ºè¿™äº›ç‰‡æ®µç´¯è®¡èµ·æ¥ä»æ— æ³•å›ç­”åŸå§‹é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´ï¼šâ€œæœ¬çŸ¥è¯†åº“é‡Œä¸åŒ…å«è¿™ä¸ªé—®é¢˜ã€‚â€
                    æŠ½å–çš„ç­”æ¡ˆç‰‡æ®µ:
                    {summaries}
                    è¯·æ ¹æ®ä»¥ä¸Šç‰‡æ®µï¼Œç»™å‡ºæœ€ç»ˆçš„è¯¦ç»†å›ç­”:
                    """
                    COMBINE_PROMPT = PromptTemplate(
                        template=combine_prompt_template, input_variables=["summaries"]
                    )

                    model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)
                    chain = load_qa_chain(
                        llm=model,
                        chain_type="map_reduce",
                        question_prompt=QUESTION_PROMPT,
                        combine_prompt=COMBINE_PROMPT
                    )

                    response_obj = chain.invoke(
                        {"input_documents": docs, "question": user_question},
                        return_only_outputs=True
                    )
                    response = response_obj['output_text']

                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

# --- ç‰ˆæƒä¿¡æ¯ ---
st.markdown("---")
footer_text = "Copyright Â© 2025-   ç‰ˆæƒæ‰€æœ‰ï¼šåä¸­å¸ˆèŒƒå¤§å­¦æ²ˆå¨ï¼Œé‚®ç®±ï¼šsw@ccnu.edu.cn"
st.markdown(f"<div style='text-align: center; color: grey;'>{footer_text}</div>", unsafe_allow_html=True)
