import streamlit as st
import os
import docx
import time
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain_core.prompts import PromptTemplate
import google.generativeai as genai

# --- å¸¸é‡å®šä¹‰ ---
FAISS_INDEX_PATH = "faiss_index"

# --- UIç•Œé¢è®¾è®¡ ---
st.set_page_config(page_title="ç°ä»£æ±‰è¯­è¯­æ³•ä¸‰ç™¾é—®æ™ºæ€ä½“æ£€ç´¢ç³»ç»Ÿ", layout="wide")
st.title("ğŸ“– ç°ä»£æ±‰è¯­è¯­æ³•ä¸‰ç™¾é—®æ™ºæ€ä½“æ£€ç´¢ç³»ç»Ÿ")

# ====================================================================
# æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨CSSæ³¨å…¥æ¥åˆ›å»ºå›ºå®šåœ¨ä¸»é¡µé¢åº•éƒ¨çš„é¡µè„š
# ====================================================================
footer_css = """
<style>
.footer {
    position: fixed;
    left: 0;
    bottom: 0;
    width: 100%;
    background-color: var(--background-color); /* é€‚åº”Streamlitä¸»é¢˜èƒŒæ™¯è‰² */
    color: var(--text-color); /* é€‚åº”Streamlitä¸»é¢˜æ–‡å­—é¢œè‰² */
    text-align: center;
    padding: 0.5rem;
    /* ä¸ºä¾§è¾¹æ ç•™å‡ºç©ºé—´ï¼Œé¿å…é¡µè„šæ–‡å­—è¢«ä¾§è¾¹æ é®æŒ¡ */
    padding-left: 21rem; 
    z-index: 99;
}
/* å½“å±å¹•å®½åº¦è¾ƒå°æ—¶ï¼ˆä¾‹å¦‚æ‰‹æœºï¼‰ï¼Œä¾§è¾¹æ ä¼šæ”¶èµ·ï¼Œæ­¤æ—¶ä¸å†éœ€è¦å·¦è¾¹è· */
@media (max-width: 768px) {
    .footer {
        padding-left: 0.5rem;
    }
}
</style>
"""

footer_html = """
<div class="footer">
    <p>Copyright Â© 2025-&nbsp;&nbsp;&nbsp;ç‰ˆæƒæ‰€æœ‰ï¼šåä¸­å¸ˆèŒƒå¤§å­¦æ²ˆå¨&nbsp;&nbsp;&nbsp;é‚®ç®±ï¼šsw@ccnu.edu.cn</p>
</div>
"""
st.markdown(footer_css, unsafe_allow_html=True)
st.markdown(footer_html, unsafe_allow_html=True)

# --- API KeyåŠæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿæ ¸å¿ƒè®¾ç½®")
    api_key_input = st.text_input(
        "è¯·è¾“å…¥æ‚¨çš„Google API Key:",
        type="password",
        help="ç³»ç»Ÿä¹Ÿä¼šè‡ªåŠ¨æ£€æµ‹'GOOGLE_API_KEY'ç¯å¢ƒå˜é‡ã€‚"
    )
    # æ­¤å¤„çš„ç‰ˆæƒä¿¡æ¯å·²è¢«ç§»å‡º
    final_api_key = api_key_input if api_key_input else os.getenv("GOOGLE_API_KEY")

if not final_api_key:
    st.error("âŒ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥æ‚¨çš„Google API Keyæˆ–è®¾ç½®'GOOGLE_API_KEY'ç¯å¢ƒå˜é‡ã€‚")
    st.stop()

try:
    genai.configure(api_key=final_api_key)
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=final_api_key,
        task_type="RETRIEVAL_QUERY"
    )

except Exception as e:
    st.error(f"API Keyé…ç½®æˆ–æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„Keyæ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    st.stop()


# --- çŸ¥è¯†åº“å¤„ç†æ ¸å¿ƒå‡½æ•° (ä¼˜åŒ–ç‰ˆï¼ŒåŒ…å«åˆ†æ‰¹å¤„ç†å’Œé‡è¯•) ---
def process_and_save_document(uploaded_file):
    with st.spinner("é¦–æ¬¡åˆå§‹åŒ–ï¼šæ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºå‘é‡çŸ¥è¯†åº“..."):
        try:
            doc = docx.Document(uploaded_file)
            full_text = "\n".join([para.text for para in doc.paragraphs if para.text])

            if not full_text.strip():
                st.error("ä¸Šä¼ çš„æ–‡æ¡£ä¸ºç©ºæˆ–ä¸åŒ…å«ä»»ä½•æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥åé‡æ–°ä¸Šä¼ ã€‚")
                return

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(full_text)

            if not chunks:
                st.error("æ–‡æ¡£åˆ‡åˆ†åæœªäº§ç”Ÿä»»ä½•æ–‡æœ¬å—ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹ã€‚")
                return

            batch_size = 100

            st.info(f"æ£€æµ‹åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚å¼€å§‹åˆ†æ‰¹æ„å»ºçŸ¥è¯†åº“...")
            vector_store = FAISS.from_texts(
                texts=chunks[:batch_size],
                embedding=embeddings
            )
            st.success(f"å·²æˆåŠŸå¤„ç†å‰ {min(batch_size, len(chunks))}/{len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

            for i in range(batch_size, len(chunks), batch_size):
                retries = 3
                while retries > 0:
                    try:
                        batch = chunks[i:i + batch_size]
                        vector_store.add_texts(texts=batch)
                        st.success(f"å·²æˆåŠŸå¤„ç† {min(i + batch_size, len(chunks))}/{len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
                        time.sleep(1)
                        break
                    except Exception as e:
                        retries -= 1
                        st.warning(f"å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}ã€‚å‰©ä½™é‡è¯•æ¬¡æ•°: {retries}ã€‚æ­£åœ¨ç­‰å¾…åé‡è¯•...")
                        time.sleep(5)
                if retries == 0:
                    st.error(f"æ‰¹æ¬¡ {i // batch_size + 1} é‡è¯•å¤šæ¬¡åä»ç„¶å¤±è´¥ï¼ŒçŸ¥è¯†åº“æ„å»ºä¸­æ­¢ã€‚")
                    raise Exception("Failed to process document in batches.")

            vector_store.save_local(FAISS_INDEX_PATH)
            st.info("æ‰€æœ‰æ–‡æœ¬å—å¤„ç†å®Œæ¯•ï¼Œå‘é‡çŸ¥è¯†åº“å·²æˆåŠŸä¿å­˜ï¼")

        except Exception as e:
            st.error(f"å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            if os.path.exists(FAISS_INDEX_PATH):
                import shutil
                shutil.rmtree(FAISS_INDEX_PATH)


@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½å‘é‡çŸ¥è¯†åº“...")
def load_vector_store():
    try:
        return FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
    except Exception as e:
        st.error(f"åŠ è½½æœ¬åœ°å‘é‡çŸ¥è¯†åº“å¤±è´¥: {e}. å¦‚æœæ‚¨åˆšåˆšä¸Šä¼ äº†æ–°æ–‡ä»¶ï¼Œè¯·åˆ·æ–°é¡µé¢ã€‚")
        return None


# --- ä¸»é€»è¾‘ ---
vector_store = None

if os.path.exists(FAISS_INDEX_PATH):
    st.sidebar.success("âœ… å‘é‡çŸ¥è¯†åº“å·²åŠ è½½ï¼Œç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
    vector_store = load_vector_store()
else:
    st.sidebar.warning("âš ï¸ ç³»ç»Ÿä¸­æœªæ‰¾åˆ°å‘é‡çŸ¥è¯†åº“ã€‚")
    st.sidebar.markdown("---")
    st.sidebar.header("é¦–æ¬¡åˆå§‹åŒ–")
    uploaded_file = st.sidebar.file_uploader(
        "è¯·ä¸Šä¼ åˆå§‹çš„WordçŸ¥è¯†åº“æ–‡æ¡£ (.docx)",
        type="docx"
    )

    if uploaded_file is not None:
        process_and_save_document(uploaded_file)
        st.sidebar.success("çŸ¥è¯†åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜ï¼é¡µé¢å°†è‡ªåŠ¨åˆ·æ–°ä»¥åŠ è½½ã€‚")
        st.rerun()
    else:
        st.info("è¯·â€œç®¡ç†å‘˜â€åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ªWordæ–‡æ¡£æ¥æ„å»ºç³»ç»ŸçŸ¥è¯†åº“ã€‚")
        st.stop()

# --- èŠå¤©ç•Œé¢ ---
if vector_store:
    # ä¸ºäº†ç»™å›ºå®šçš„é¡µè„šç•™å‡ºç©ºé—´ï¼Œå¯ä»¥åœ¨èŠå¤©å®¹å™¨åº•éƒ¨å¢åŠ ä¸€ä¸ªç©ºç™½åŒºåŸŸ
    st.container().empty()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # åœ¨èŠå¤©ç•Œé¢çš„æœ€åï¼Œå¢åŠ ä¸€ä¸ªä¸é¡µè„šç­‰é«˜çš„ç©ºç™½divï¼Œé˜²æ­¢æœ€åä¸€æ¡æ¶ˆæ¯è¢«é¡µè„šé®æŒ¡
    if len(st.session_state.messages) > 0:
        st.markdown("<div style='padding-bottom: 4rem;'></div>", unsafe_allow_html=True)

    if user_question := st.chat_input("è¯·å°±ç°ä»£æ±‰è¯­è¯­æ³•è¿›è¡Œæé—®..."):
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– æ™ºæ€ä½“æ­£åœ¨æ£€ç´¢ä¸æ€è€ƒä¸­..."):
                docs = vector_store.similarity_search(user_question, k=3)

                if not docs:
                    response = "æœ¬çŸ¥è¯†åº“é‡Œä¸åŒ…å«è¿™ä¸ªé—®é¢˜ã€‚"
                else:
                    question_prompt_template = """
                    è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚
                    ç¡®ä¿ä½ çš„å›ç­”å®Œå…¨åŸºäºè¿™äº›ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€‚
                    å¦‚æœæ ¹æ®ä¸Šä¸‹æ–‡æ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·æŒ‡æ˜ä¿¡æ¯ä¸è¶³ã€‚
                    ä¸Šä¸‹æ–‡:
                    {context}
                    é—®é¢˜:
                    {question}
                    å›ç­”:
                    """
                    QUESTION_PROMPT = PromptTemplate(
                        template=question_prompt_template, input_variables=["context", "question"]
                    )

                    combine_prompt_template = """
                    ç°æœ‰å¤šä¸ªä»æ–‡æ¡£ä¸­æŠ½å–çš„ç­”æ¡ˆç‰‡æ®µï¼Œè¯·å¯¹è¿™äº›ç‰‡æ®µè¿›è¡Œå…¨é¢å’Œè¯¦ç»†çš„æ•´åˆä¸æ€»ç»“ï¼Œå½¢æˆä¸€ä¸ªæœ€ç»ˆçš„ã€æµç•…ä¸”å”¯ä¸€çš„å›ç­”ã€‚
                    è¯·ä¸“æ³¨äºæ•´åˆä¿¡æ¯ï¼Œä¸è¦é‡å¤â€œæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡â€ä¹‹ç±»çš„è¯è¯­ã€‚
                    å¦‚æœä½ è®¤ä¸ºè¿™äº›ç‰‡æ®µç´¯è®¡èµ·æ¥ä»æ— æ³•å›ç­”åŸå§‹é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´ï¼šâ€œæœ¬çŸ¥è¯†åº“é‡Œä¸åŒ…å«è¿™ä¸ªé—®é¢˜ã€‚â€
                    æŠ½å–çš„ç­”æ¡ˆç‰‡æ®µ:
                    {summaries}
                    è¯·æ ¹æ®ä»¥ä¸Šç‰‡æ®µï¼Œç»™å‡ºæœ€ç»ˆçš„è¯¦ç»†å›ç­”:
                    """
                    COMBINE_PROMPT = PromptTemplate(
                        template=combine_prompt_template, input_variables=["summaries"]
                    )

                    model = ChatGoogleGenerativeAI(
                        model="gemini-2.0-flash",
                        google_api_key=final_api_key,
                        temperature=0.3
                    )

                    chain = load_qa_chain(
                        llm=model,
                        chain_type="map_reduce",
                        question_prompt=QUESTION_PROMPT,
                        combine_prompt=COMBINE_PROMPT
                    )

                    response_obj = chain.invoke(
                        {"input_documents": docs, "question": user_question},
                        return_only_outputs=True
                    )
                    response = response_obj['output_text']

                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                # å“åº”åç«‹å³é‡æ–°è¿è¡Œï¼Œä»¥ç¡®ä¿åº•éƒ¨çš„ç©ºç™½å¡«å……ç‰©è¢«æ¸²æŸ“
                st.rerun()
